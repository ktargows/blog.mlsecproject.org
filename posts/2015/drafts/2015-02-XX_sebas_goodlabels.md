
The Importance of Using Good Labels in your Datasets

As security researchers we often create new machine learning algorithms to solve problems; what lead us to start a quest to **obtain a good dataset**. It may be that we are trying to detect malware, identify attacks or analyze IDS logs, but at some point we figure it out that we need a dataset to complete our task. But not any dataset; in fact we need **a labeled dataset**. The dataset will be used not only to learn the features of, for example, malware traffic, but also to verify how good our algorithm is. Since getting a dataset is difficult and time consuming, the most common solution is to get a third-party dataset created by someone else; although some researchers with time and resources may create their own. Either way, most usually we obtain a dataset of malware traffic (continuing with the malware traffic detection example) and we assign the label _Malware_ to all of its instances. This looks good, so we make our training and testing, we obtain results and we publish. However, there are important **problems in this approach** that can **jeopardize the results** completely. Using bad quality or wrong labels in a dataset may not only impact the verification of the results, but may strongly influence the performance of the algorithm. Let's talk about each issue in turn.

## Insufficient Types of Labels
The first problem is that of insufficient types of labels. It normally happens that we assign labels based on the origin of the data: _if it was a malware capture, everything is malware_. This is good for looking at the candidate malware features and maybe run some tests, but without **normal** labels we lack a serious amount of information. Using only the _positive class_ is not enough for any research. An algorithm that uses a dataset with only the positive class (malware labels in our example) lacks the possibility to compute some errors values. We will be able to compute **True Positives** and **False Negatives**, but we can **not** compute **False Positives** or **True Negatives**. Therefore, we can not compute any error metric such as True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, F-Measure, Error Rate, Precision, Accuracy, etc. Even ROC curves are out of scope. In this context, it is impossible to really verify the performance of an algorithm without all the types of labels; even an unsupervised algorithm. A good dataset should have labels for the positive class, e.g. **Malware**, the negative class, e.g. **Normal** and **Background** labels.

## Getting Normal Labels
To compute all the error metrics **we need** labeled instances for the **positive** and the **negative class**. In our example they are Malware and Normal labels. If obtaining a malware dataset is difficult, obtaining a normal dataset is far more difficult. To generate this type of dataset is **not enough to execute a _Normal_ computer for some time** and capture the features we need. We should think about what a normal computer may do in the real environment where our algorithm is supposed to work. A normal computer may need to check email, use P2P, chat with partners, use the network, etc. The exact data that we should capture depends on which context our algorithm was designed for. If, for example, our algorithm is designed to protect the average group of home Facebook Windows users, then we may expect that the normal traffic of some advance network administration tools may introduce some false positives in our experiments.

> If obtaining a malware dataset is difficult, obtaining a normal dataset is far more difficult.

Another issue with obtaining normal data is that **it is more time consuming** than obtaining malware data. We can repeat the capture of Malware data, but we **can not automate** the capture of normal data. We can not just sit down in a machine and type _as_ a normal user; because our behavior will not be exactly normal and we will be adding a bias to our dataset. Ideally, the best would be to really capture normal users working. However, this may pose some difficulties, since **we need to check that the normal users are really normal**. This verification may involve checking the normal computers for infections or even ask the users what actions they are usually doing. Remember that a lot of Universities and companies have security teams making attacks. In the other side, if we need to execute some program on their computers to get the data we need to be sure that we have authorization and the consent of the users. We should respect her/his privacy and explain what we plan to use the data for. In summary, we should **get normal data that is representative of our working context** and we should make sure that the data is in fact **coming from normal sources**.

## About Background Labels
When we discussed the types of labels we said that we need background labels. This type of label corresponds to data for which we do not know its origin. In the malware traffic example it corresponds to unknown traffic that we don't know if it is malware or normal. First, having this type of traffic **force us to differentiate** between what is malware or normal, and what is not. Second, this traffic is meant to be mixed with the positive and negative labels in order **to confuse and make things harder for our algorithm**. It is **not** the same to find 10 malware flows mixed with 10 normal flows, than to find 10 malware flows mixed with 10 normal flows and **1,000,000 unknown** flows. The background labels may not be used in some algorithms, but they are a good way of getting a real context and a good way of balancing the dataset.


## Origin of Labels and How Specific They Are
Even with positive and negative class labels in our dataset we should think about the origin of the traffic and how specific our labels should be. Consider the case of normal labels in the example of malware network traffic detection. We may be tempted to assign the normal label to **all the traffic coming from the normal IP address** of the normal computer. However, in doing so, we are making some assumptions that may result **false**. First, we are _assuming_ that all the traffic from the normal IP is normal. However, most of the times an IP address receives traffic from a large number of unknown computers in a network. For example, **it may receive attacks from computers in the Internet**, receive attacks from the internal network, it may receive broadcast NetBIOS request from infected computers, it can have its ports scanned, or it can just receive ARP packets from a incorrectly configured router. **All this traffic is coming to and from the normal IP, but it is certainly not a result of a normal action**. Even more, some traffic is being generated from the normal computer in response to these requests, but that traffic was also not generated as a result of a normal user activity. The same concept of origin of traffic is true for the Malware traffic. A computer infected with the Zeus malware may be still attacked by an external Bobax bot.

> It is more difficult to put correct labels than to obtain a dataset.

This means that it may be a good idea to **differentiate** all least between the **traffic going to** the IP address and the **traffic coming from** the IP address. This may work as a first attempt at separating the normal and malware traffic from the unknown traffic.


## Unavoidable Errors in the Labels
Even with a very careful labeling process we should be aware that there will be some errors in the labels. We already saw that a normal traffic may be not completely normal and that the malware traffic may include normal traffic. A good idea may be to write down all the possibilities and try to determine why such a situation may arise in the dataset. Still using the malware traffic example for convenience, we may want to find:

- Malware traffic labeled as Normal.
    - Because the machine was not clean from the beginning.
- Background traffic labeled as Normal.
    - For example external attacks or port scans.
- Normal traffic labeled as Malware.
    - Usually because the traffic generated by the infected computer is being automatically generated by the Operating System.
- Background traffic labeled as Malware.
    - Attacks from Internet to the infected computers.

Listing these errors may help identify possible sources of problems in the experiments and help obtain a better labeled dataset.

## The Dynamisms of Labels
It may be the case that the assignment of labels is done with the **assumption that the instance that was labeled Normal, should always be labeled normal**. In the malware traffic example it means that an IP address that was labeled in one way, can not change its label. We simply did not consider the possibility, but mostly because it would be too difficult to implement. In fact, a more real dataset would have a **normal IP address** that generates traffic from some time, then **starts generating malware** traffic for some days and **finally generates normal traffic again**. This would be a very realistic environment to test our algorithms with a normal computer that gets infected and it is cleaned later. In such an environment we should think about **which** label this IP may get, but most importantly we should decide **when should it get the label**. This suggests that the relationship between labels and instances in the dataset may not be unique.

## Balance of the Datasets
Our last problem to consider is the balance of the dataset. It is usually the case that we have more data with the positive label than with the negative label. This is due to the difficulty of obtaining normal data. However, in a real network most of the traffic is normal and some small percentage is malicious. This gives us the intuition that to be real, our dataset should be correctly balanced. Although this is related with the [base rate fallacy](http://en.wikipedia.org/wiki/Base_rate_fallacy), it suggests that a **wrongly balanced** dataset may result in **biased** error metrics. The balance issue highlights the difficulty of obtaining a large amount of normal traffic.

## A Labeling Methodology
To avoid most of these errors it could be a good idea **to have a labelling methodology**. This may include a way of testing for the origin of data, the IP addresses, the malware types, the amount of labels, etc. In particular a labeling methodology can result to assign labels in a dataset. For example, the [ralabel](http://nsmwiki.org/Argus) tool of the Argus suite is usually used to label network flows using rules. A labeling methodology can help us obtain good results if we do it properly. Among the things that we should verify in our methodology are if the labels should be unique or not, if the order of assignment matters and if we use the same rules for all our datasets. For example, a problem with the order in which the rules are applied may result in an IP address being labeled Malware first and then mistakenly being labeled Normal by another rule. If we don't check the rules, we may never found the error.

A **wrongly labeled** dataset may easily result in algorithms using the wrong training features, in an insufficient number of metrics reported, in **highly biased detection results**, in **wrong performance metrics** and in a **mistaken idea** of how good the algorithm is. The task of correctly labeling our dataset may seem time consuming but it can teach us a lot about our data and it will provide us with a strong base to evaluate our algorithm.
